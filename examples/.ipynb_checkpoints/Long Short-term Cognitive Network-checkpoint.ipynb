{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-term Cognitive Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, sys, random, time\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import MultiOutputMixin, BaseEstimator\n",
    "\n",
    "from hampel import hampel\n",
    "\n",
    "import lstcn\n",
    "from lstcn.LSTCN import LSTCN\n",
    "from lstcn.STCN import STCN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_random_seeds():\n",
    "    os.environ['PYTHONHASHSEED']=str(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source, n_steps, split=0.8):\n",
    "\n",
    "    \"\"\" Prepare the time series for learning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source  :   {string} path to the CSV (variables must appear by column)\n",
    "    n_steps :   {int} Number of steps-ahead to be forecast.\n",
    "    split   :   {float} Proportion of data used for training.\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train, Y_train, X_test, Y_test, n_features\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(source, na_values='?')\n",
    "    n_features = len(df.columns)\n",
    "\n",
    "    for col in df.columns: \n",
    "\n",
    "        # imputing missing values using kNN\n",
    "        df[col].interpolate(method='nearest', inplace=True)\n",
    "        df[col] = df[col].astype('float64')\n",
    "\n",
    "        # removing outliers to obtain more realistic errors\n",
    "        df[col] = hampel(df[col], window_size=5, n=3, imputation=True)\n",
    "\n",
    "        # normalize dataset to facilitate error analysis\n",
    "        df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "    data = df.to_numpy()\n",
    "    # splitting the data to create the datasets\n",
    "    data_train = data[:(int(split * data.shape[0])) + n_steps,:]\n",
    "    data_test = data[-int((1-split) * data.shape[0]):,:]\n",
    "\n",
    "    # creating X_train, Y_train, X_test, Y_test\n",
    "    X_train, Y_train = create_dataset(data_train, n_features, n_steps)\n",
    "    X_test, Y_test = create_dataset(data_test, n_features, n_steps)\n",
    "\n",
    "    return data_train, X_train, Y_train, X_test, Y_test, n_features\n",
    "\n",
    "def create_dataset(data, n_features, n_steps):\n",
    "    \n",
    "    \"\"\" Create X and Y from a portion of the time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data         :   {array-like} Portion of the time series.\n",
    "    n_features   :   {int} Number of features in the time series. \n",
    "    n_steps      :   {int} Number of steps-ahead to be forecast.\n",
    "    Returns\n",
    "    ----------\n",
    "    X_train, Y_train or X_test, Y_test\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X_data = []\n",
    "    Y_data = []\n",
    "\n",
    "    for index in range(0, data.shape[0]-(2*n_steps-1)):\n",
    "\n",
    "        # the moving windows is set to one\n",
    "        X_data.append(data[index:(index + n_steps),:])\n",
    "        Y_data.append(data[(index + n_steps):(index + 2*n_steps),:])\n",
    "\n",
    "    # reshape data into 2D-NumPy arrays\n",
    "    X = np.reshape(np.array(X_data), (len(X_data), n_steps*n_features))\n",
    "    Y = np.reshape(np.array(Y_data), (len(Y_data), n_steps*n_features))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data'\n",
    "with open(\"output.csv\", \"w\") as output:\n",
    "\n",
    "    # printing the header of the output file\n",
    "    output.write('dataset,train,test,step,time\\n')\n",
    "    n_datasets = len(os.listdir(folder))\n",
    "\n",
    "    for dataset in os.listdir(folder):\n",
    "\n",
    "        for n_steps in range(1,6):\n",
    "\n",
    "            data, X_train, Y_train, X_test, Y_test, n_features = load_data(\n",
    "                folder + os.sep + dataset, n_steps)\n",
    "\n",
    "            param_search = {\n",
    "                'alpha': [1.0E-4, 1.0E-2, 1.0, 1.0E+2, 1.0E+4],\n",
    "                'n_blocks' : range(1,11)\n",
    "            }\n",
    "\n",
    "            reset_random_seeds()\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            model = LSTCN(n_features, n_steps)\n",
    "\n",
    "            start = time.time()\n",
    "            scorer = make_scorer(model.score, greater_is_better=False)\n",
    "            gsearch = GridSearchCV(estimator=model, cv=tscv, param_grid=param_search, refit=True,\n",
    "                                   n_jobs=-1, error_score='raise', scoring=scorer)\n",
    "\n",
    "            gsearch.fit(X_train, Y_train)\n",
    "            best_model = gsearch.best_estimator_\n",
    "            end = time.time()\n",
    "\n",
    "            train_error = round(best_model.score(best_model.predict(X_train), Y_train),4)\n",
    "            test_error = round(best_model.score(best_model.predict(X_test), Y_test),4)\n",
    "\n",
    "            output.write(dataset + ',' + str(train_error) + ',' + str(test_error) + ',')\n",
    "            output.write(str(n_steps) + ',' + str(end-start) + '\\n')\n",
    "            output.flush()\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
